{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Neural Creative Transfer\n",
    "\n",
    "We implement an modification of Gatys, Ecker and Bethge's [neural style transfer algorithm](http://arxiv.org/abs/1508.06576) for our creative assets.\n",
    "\n",
    "The algorith uses a VGG-19 network to separate and recombine content of an creative and style of a website, and therefore provides a system for creating native creative for advertising. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import scipy.io\n",
    "import scipy.misc\n",
    "import tensorflow as tf \n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import imshow\n",
    "from PIL import Image\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# Constants\n",
    "###############################################################################\n",
    "# Weight to put more emphasis on the content loss.\n",
    "ALPHA = 5\n",
    "# Weight to put more emphasis on the style loss.\n",
    "BETA = 1000\n",
    "# Max iteration to run the optimizer\n",
    "MAX_ITERATION = 1000\n",
    "#Number of iterations between optimizer print statements\n",
    "PRINT_ITERATION = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def setup_network_from_file(path, image):\n",
    "    \"\"\"\n",
    "    We start with the pre-trained VGG model from the paper \"Very Deep Convolutional\n",
    "    Networks for Large-Scale Visual Recognition\". We  strip out all the conv, relu\n",
    "    and pool layers from the VGG-19 network and replace the maxpool layers with \n",
    "    average pool layers as suggested by the authors in the \"A Neural Algorithm of \n",
    "    Artistic Style\" paper. Here is the configuration for our modified VGG network \n",
    "    (layer : filter height, filter width, input channels, output channels):\n",
    "        conv1_1  :  3  3  3    64\n",
    "        conv1_2  :  3  3  64   64 \n",
    "        avgpool  :\n",
    "        conv2_1  :  3  3  64   128  \n",
    "        conv2_2  :  3  3  128  128 \n",
    "        avgpool  :\n",
    "        conv3_1  :  3  3  128  256 \n",
    "        conv3_2  :  3  3  256  256\n",
    "        conv3_3  :  3  3  256  256\n",
    "        conv3_4  :  3  3  256  256\n",
    "        avgpool  :\n",
    "        conv4_1  :  3  3  256  512  \n",
    "        conv4_2  :  3  3  512  512\n",
    "        conv4_3  :  3  3  512  512\n",
    "        conv4_4  :  3  3  512  512\n",
    "        avgpool  :\n",
    "        conv5_1  :  3  3  512  512 \n",
    "        conv5_2  :  3  3  512  512 \n",
    "        conv5_3  :  3  3  512  512 \n",
    "        conv5_4  :  3  3  512  512 \n",
    "        avgpool  :    \n",
    "    \"\"\"\n",
    "    \n",
    "    _, image_height, image_width, color_channels = image.shape\n",
    "    \n",
    "    # Strip out the layers from the VGG-19 model \n",
    "    vgg = scipy.io.loadmat(path)['layers'] \n",
    "    \n",
    "    def _weights(layer):\n",
    "        \"\"\"\n",
    "        Extract the weights at a given layer.\n",
    "        \"\"\"\n",
    "        W = vgg[0][layer][0][0][2][0][0]\n",
    "        return tf.constant(W)\n",
    "    \n",
    "    def _bias(layer):\n",
    "        \"\"\"\n",
    "        Extract the bias at a given layer.\n",
    "        \"\"\"\n",
    "        b = vgg[0][layer][0][0][2][0][1]\n",
    "        return tf.constant(np.reshape(b, (b.size)))\n",
    "\n",
    "    def _conv2d(input_layer, layer):\n",
    "        \"\"\"\n",
    "        Construct the conv2d layer.\n",
    "        \"\"\"\n",
    "        W = _weights(layer)\n",
    "        b = _bias(layer)\n",
    "        return tf.nn.conv2d(input_layer, filter=W, strides=[1, 1, 1, 1], padding='SAME') + b\n",
    "    \n",
    "    def _relu(input_layer):\n",
    "        \"\"\"\n",
    "        Construct the relu layer.\n",
    "        \"\"\"\n",
    "        return tf.nn.relu(input_layer)\n",
    "\n",
    "    def _avg_pool(input_layer):\n",
    "        \"\"\"\n",
    "        Return the average pooling layer.\n",
    "        \"\"\"\n",
    "        return tf.nn.avg_pool(input_layer, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "\n",
    "    # Set up the modifiled VGG network.  \n",
    "    network = {}\n",
    "    network['input']    = tf.Variable(np.zeros((1, image_height, image_width, color_channels)), dtype = np.float32)\n",
    "    network['conv1_1']  = _relu(_conv2d(network['input'], 0))\n",
    "    network['conv1_2']  = _relu(_conv2d(network['conv1_1'], 2))    \n",
    "    network['avgpool1'] = _avgpool(network['conv1_2'])\n",
    "    network['conv2_1']  = _relu(_conv2d(network['avgpool1'], 5))\n",
    "    network['conv2_2']  = _relu(_conv2d(network['conv2_1'], 7))\n",
    "    network['avgpool2'] = _avgpool(network['conv2_2'])\n",
    "    network['conv3_1']  = _relu(_conv2d(network['avgpool2'], 10))\n",
    "    network['conv3_2']  = _relu(_conv2d(network['conv3_1'], 12))\n",
    "    network['conv3_3']  = _relu(_conv2d(network['conv3_2'], 14))\n",
    "    network['conv3_4']  = _relu(_conv2d(network['conv3_3'], 16))\n",
    "    network['avgpool3'] = _avgpool(network['conv3_4'])\n",
    "    network['conv4_1']  = _relu(_conv2d(network['avgpool3'], 19))\n",
    "    network['conv4_2']  = _relu(_conv2d(network['conv4_1'], 21))\n",
    "    network['conv4_3']  = _relu(_conv2d(network['conv4_2'], 23))\n",
    "    network['conv4_4']  = _relu(_conv2d(network['conv4_3'], 25))\n",
    "    network['avgpool4'] = _avgpool(network['conv4_4'])\n",
    "    network['conv5_1']  = _relu(_conv2d(network['avgpool4'], 28))\n",
    "    network['conv5_2']  = _relu(_conv2d(network['conv5_1'], 30))\n",
    "    network['conv5_3']  = _relu(_conv2d(network['conv5_2'], 32))\n",
    "    network['conv5_4']  = _relu(_conv2d(network['conv5_3'], 34))\n",
    "    network['avgpool5'] = _avgpool(network['conv5_4'])\n",
    "    \n",
    "    return network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def content_loss_func(sess, network, image, layers = [], weights = []):\n",
    "    \"\"\"\n",
    "    Content loss function.\n",
    "    \"\"\"\n",
    "    def _content_loss(p, x):\n",
    "        _, height, width, depth = p.get_shape()\n",
    "        # N is the number of the output channels.\n",
    "        N = depth.value\n",
    "        # M is the size of the feature map.\n",
    "        M = height.value * width.value\n",
    "        # I use a similar normalization constant to the one used in\n",
    "        # style loss instead of the one in the paper, which is 0.5\n",
    "        C = 1. / (2 * N * M)\n",
    "        return C * tf.reduce_sum(tf.pow(x - p, 2))\n",
    "    \n",
    "    sess.run(network['input'].assign(image))\n",
    "    content_layers = zip(layers, weights)\n",
    "    content_loss = sum([_content_loss(sess.run(network[layer]), network[layer]) * weight\n",
    "                        for layer, weight in style_layers])\n",
    "        \n",
    "    return content_loss / len(layers)\n",
    "\n",
    "def style_loss_func(sess, network, image, layers = [], weights = []):\n",
    "    \"\"\"\n",
    "    Style loss function as defined in the paper.\n",
    "    \"\"\"\n",
    "    def _gram_matrix(x, N, M):\n",
    "        \"\"\"\n",
    "        The gram matrix.\n",
    "        \"\"\"\n",
    "        F = tf.reshape(x, (M, N))\n",
    "        return tf.matmul(tf.transpose(F), F)\n",
    "\n",
    "    def _style_loss(a, x):\n",
    "        \"\"\"\n",
    "        The style loss calculation.\n",
    "        \"\"\"\n",
    "        _, height, width, depth = a.get_shape()\n",
    "        # N is the number of the output channels.\n",
    "        N = depth.value\n",
    "        # M is the size of the feature map.\n",
    "        M = height.value * width.value\n",
    "        # A is the style representation of the original image.\n",
    "        A = _gram_matrix(a, N, M)\n",
    "        # G is the style representation of the generated image.\n",
    "        G = _gram_matrix(x, N, M)\n",
    "        # normalization factor\n",
    "        C = 1. / (4 * N**2 * M**2)\n",
    "\n",
    "        return C * tf.reduce_sum(tf.pow(G - A, 2))\n",
    "\n",
    "    sess.run(network['input'].assign(image))\n",
    "    style_layers = zip(layers, weights)\n",
    "    style_loss = sum([_style_loss(sess.run(network[layer]), network[layer]) * weight\n",
    "                      for layer, weight in style_layers])\n",
    "    \n",
    "    return style_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def style_transfer(content_img, style_img, init_img, **kwargs):\n",
    "  with tf.Session() as sess:\n",
    "    \n",
    "    assert content_img.shape == style_imgs.shape\n",
    "    \n",
    "    # setup the modifield VGG network\n",
    "    network = setup_network_from_file(content_img)\n",
    "    \n",
    "    # content loss\n",
    "    L_content = content_loss_func(sess, network, content_img)\n",
    "    \n",
    "    # style loss\n",
    "    L_style = style_loss_func(sess, network, style_img)\n",
    "    \n",
    "    # loss weights\n",
    "    alpha = kwargs.get('alpha', ALPHA)\n",
    "    beta  = kwargs.get('beta', BETA)\n",
    "    \n",
    "    # total loss\n",
    "    L_total  = alpha * L_content + beta  * L_style\n",
    "    \n",
    "    # 'L-BFGS-B' requires much less tuning compared to 'Adam'\n",
    "    optimizer = tf.contrib.opt.ScipyOptimizerInterface(\n",
    "        L_total, \n",
    "        method='L-BFGS-B',\n",
    "        options={'maxiter': kwargs.get('maxiter', MAX_ITERATION),\n",
    "                 'disp': kwargs.get('maxiter', PRINT_ITERATION)})\n",
    "    \n",
    "    # Optimizing the loss function\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    sess.run(init_op)\n",
    "    sess.run(net['input'].assign(init_img))\n",
    "    optimizer.minimize(sess)\n",
    "    \n",
    "    output_img = sess.run(net['input'])\n",
    "    \n",
    "    return output_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
